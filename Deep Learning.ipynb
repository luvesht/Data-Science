{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4: Deep Learning\n",
    "\n",
    "Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neural network (RNN) on Keras:\n",
    "The most commonly used neural network for predicting time series data is Recurrent neural network (RNN) on Keras.\n",
    "\n",
    "The recurrent neural network (RNN) is a type of artificial neural network with self-loop in its hidden layer(s), which enables RNN to use the previous state of the hidden neuron(s) to learn the current state given the new input. RNN is good at processing sequential data. Long short-term memory (LSTM) cell is a specially designed working unit that helps RNN better memorize the long-term context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\program files\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\program files\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\program files\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\program files\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in c:\\program files\\anaconda3\\lib\\site-packages (from keras)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import h5py\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "df = pd.read_csv('prices-split-adjusted (1).csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>WLTW</td>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>WLTW</td>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-07</th>\n",
       "      <td>WLTW</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-08</th>\n",
       "      <td>WLTW</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11</th>\n",
       "      <td>WLTW</td>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           symbol        open       close         low        high     volume\n",
       "date                                                                        \n",
       "2016-01-05   WLTW  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "2016-01-06   WLTW  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2016-01-07   WLTW  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "2016-01-08   WLTW  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "2016-01-11   WLTW  117.010002  114.970001  114.089996  117.330002  1408600.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "      <th>adj close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>WLTW</td>\n",
       "      <td>123.430000</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "      <td>125.839996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>WLTW</td>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "      <td>119.980003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-07</th>\n",
       "      <td>WLTW</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "      <td>114.949997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-08</th>\n",
       "      <td>WLTW</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "      <td>116.620003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-11</th>\n",
       "      <td>WLTW</td>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "      <td>114.970001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           symbol        open         low        high     volume   adj close\n",
       "date                                                                        \n",
       "2016-01-05   WLTW  123.430000  122.309998  126.250000  2163600.0  125.839996\n",
       "2016-01-06   WLTW  125.239998  119.940002  125.540001  2386400.0  119.980003\n",
       "2016-01-07   WLTW  116.379997  114.930000  119.739998  2489500.0  114.949997\n",
       "2016-01-08   WLTW  115.480003  113.500000  117.440002  2006300.0  116.620003\n",
       "2016-01-11   WLTW  117.010002  114.089996  117.330002  1408600.0  114.970001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['adj close'] = df['close']\n",
    "df.drop(['close'], 1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze stock data for Apple(AAPL):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "      <th>adj close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>30.490000</td>\n",
       "      <td>30.340000</td>\n",
       "      <td>30.642857</td>\n",
       "      <td>123432400.0</td>\n",
       "      <td>30.572857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>30.657143</td>\n",
       "      <td>30.464285</td>\n",
       "      <td>30.798571</td>\n",
       "      <td>150476200.0</td>\n",
       "      <td>30.625713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>30.625713</td>\n",
       "      <td>30.107143</td>\n",
       "      <td>30.747143</td>\n",
       "      <td>138040000.0</td>\n",
       "      <td>30.138571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>30.250000</td>\n",
       "      <td>29.864286</td>\n",
       "      <td>30.285715</td>\n",
       "      <td>119282800.0</td>\n",
       "      <td>30.082857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>30.042856</td>\n",
       "      <td>29.865715</td>\n",
       "      <td>30.285715</td>\n",
       "      <td>111902700.0</td>\n",
       "      <td>30.282858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 open        low       high       volume  adj close\n",
       "date                                                               \n",
       "2010-01-04  30.490000  30.340000  30.642857  123432400.0  30.572857\n",
       "2010-01-05  30.657143  30.464285  30.798571  150476200.0  30.625713\n",
       "2010-01-06  30.625713  30.107143  30.747143  138040000.0  30.138571\n",
       "2010-01-07  30.250000  29.864286  30.285715  119282800.0  30.082857\n",
       "2010-01-08  30.042856  29.865715  30.285715  111902700.0  30.282858"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.symbol == 'AAPL']\n",
    "df.drop(['symbol'],1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "      <th>adj close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.028123</td>\n",
       "      <td>0.030334</td>\n",
       "      <td>0.024806</td>\n",
       "      <td>0.244034</td>\n",
       "      <td>0.029718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.029686</td>\n",
       "      <td>0.031526</td>\n",
       "      <td>0.026268</td>\n",
       "      <td>0.302982</td>\n",
       "      <td>0.030219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.029392</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.025785</td>\n",
       "      <td>0.275875</td>\n",
       "      <td>0.025604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.025880</td>\n",
       "      <td>0.025769</td>\n",
       "      <td>0.021454</td>\n",
       "      <td>0.234989</td>\n",
       "      <td>0.025076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.023943</td>\n",
       "      <td>0.025783</td>\n",
       "      <td>0.021454</td>\n",
       "      <td>0.218903</td>\n",
       "      <td>0.026971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                open       low      high    volume  adj close\n",
       "date                                                         \n",
       "2010-01-04  0.028123  0.030334  0.024806  0.244034   0.029718\n",
       "2010-01-05  0.029686  0.031526  0.026268  0.302982   0.030219\n",
       "2010-01-06  0.029392  0.028100  0.025785  0.275875   0.025604\n",
       "2010-01-07  0.025880  0.025769  0.021454  0.234989   0.025076\n",
       "2010-01-08  0.023943  0.025783  0.021454  0.218903   0.026971"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_data(df):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    df['open'] = min_max_scaler.fit_transform(df.open.values.reshape(-1,1))\n",
    "    df['high'] = min_max_scaler.fit_transform(df.high.values.reshape(-1,1))\n",
    "    df['low'] = min_max_scaler.fit_transform(df.low.values.reshape(-1,1))\n",
    "    df['volume'] = min_max_scaler.fit_transform(df.volume.values.reshape(-1,1))\n",
    "    df['adj close'] = min_max_scaler.fit_transform(df['adj close'].values.reshape(-1,1))\n",
    "    return df\n",
    "df = normalize_data(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(stock, seq_len):\n",
    "    amount_of_features = len(stock.columns) # 5\n",
    "    data = stock.as_matrix() \n",
    "    sequence_length = seq_len + 1 # index starting from 0\n",
    "    result = []\n",
    "    \n",
    "    for index in range(len(data) - sequence_length): # maxmimum date = lastest date - sequence length\n",
    "        result.append(data[index: index + sequence_length]) # index : index + 22days\n",
    "    \n",
    "    result = np.array(result)\n",
    "    row = round(0.9 * result.shape[0]) # 90% split\n",
    "    train = result[:int(row), :] # 90% date, all features \n",
    "    \n",
    "    x_train = train[:, :-1] \n",
    "    y_train = train[:, -1][:,-1]\n",
    "    \n",
    "    x_test = result[int(row):, :-1] \n",
    "    y_test = result[int(row):, -1][:,-1]\n",
    "\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], amount_of_features))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], amount_of_features))  \n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    d = 0.3\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(Dense(32,kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(1,kernel_initializer=\"uniform\",activation='linear'))\n",
    "        \n",
    "    start = time.time()\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit test and train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.81231215e-02   3.03337678e-02   2.48062394e-02   2.44034312e-01\n",
      "    2.97178362e-02]\n",
      " [  2.96855171e-02   3.15262743e-02   2.62677938e-02   3.02982342e-01\n",
      "    3.02185399e-02]\n",
      " [  2.93917266e-02   2.80995178e-02   2.57850858e-02   2.75874854e-01\n",
      "    2.56038867e-02]\n",
      " [  2.58796844e-02   2.57693155e-02   2.14540576e-02   2.34989328e-01\n",
      "    2.50761176e-02]\n",
      " [  2.39433742e-02   2.57830226e-02   2.14540576e-02   2.18902744e-01\n",
      "    2.69707045e-02]\n",
      " [  2.72818302e-02   2.49468907e-02   2.27949303e-02   2.26868983e-01\n",
      "    2.44400840e-02]\n",
      " [  2.24611028e-02   2.21643437e-02   1.84638847e-02   2.98925222e-01\n",
      "    2.12057623e-02]\n",
      " [  2.06984135e-02   1.89843011e-02   2.00193033e-02   3.05155092e-01\n",
      "    2.51708493e-02]\n",
      " [  2.36896638e-02   2.57281929e-02   1.93890932e-02   2.10883102e-01\n",
      "    2.35198553e-02]\n",
      " [  2.47846654e-02   2.14104557e-02   2.09176943e-02   2.98711609e-01\n",
      "    1.87834049e-02]\n",
      " [  2.13126846e-02   2.32883326e-02   2.57314493e-02   3.72789542e-01\n",
      "    3.11117019e-02]\n",
      " [  3.00994958e-02   2.63861300e-02   2.62141587e-02   3.08566796e-01\n",
      "    2.66323798e-02]\n",
      " [  2.63203520e-02   2.32472100e-02   2.32105945e-02   3.06387944e-01\n",
      "    2.16794006e-02]\n",
      " [  1.92428497e-02   9.47158991e-03   1.54200927e-02   4.55488284e-01\n",
      "    7.71364371e-03]\n",
      " [  1.35407682e-02   1.36248381e-02   1.16656381e-02   5.55718551e-01\n",
      "    1.49130510e-02]\n",
      " [  1.81344769e-02   1.69008330e-02   2.37469578e-02   9.92431997e-01\n",
      "    1.87969390e-02]\n",
      " [  1.93363117e-02   1.27201694e-02   1.95500026e-02   9.13666785e-01\n",
      "    2.14222912e-02]\n",
      " [  1.67723925e-02   1.15824735e-02   1.27383473e-02   6.14463648e-01\n",
      "    9.79767512e-03]\n",
      " [  1.16311656e-02   0.00000000e+00   8.31343748e-03   6.53943906e-01\n",
      "    1.35327151e-05]\n",
      " [  0.00000000e+00   1.43923934e-03   0.00000000e+00   3.83616668e-01\n",
      "    3.62676096e-03]] 0.00515595644518\n"
     ]
    }
   ],
   "source": [
    "window = 20\n",
    "X_train, y_train, X_test, y_test = load_data(df, window)\n",
    "print (X_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation Time :  0.03902840614318848\n"
     ]
    }
   ],
   "source": [
    "model = build_model([5,window,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model with 90 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1410 samples, validate on 157 samples\n",
      "Epoch 1/90\n",
      "1410/1410 [==============================] - 7s 5ms/step - loss: 0.2445 - acc: 7.0922e-04 - val_loss: 0.4383 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.1318 - acc: 7.0922e-04 - val_loss: 0.0478 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0632 - acc: 0.0014 - val_loss: 0.0183 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0504 - acc: 0.0014 - val_loss: 0.0968 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0430 - acc: 7.0922e-04 - val_loss: 0.1248 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0390 - acc: 7.0922e-04 - val_loss: 0.0553 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0184 - acc: 0.0014 - val_loss: 0.0055 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0165 - acc: 0.0014 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0084 - acc: 0.0014 - val_loss: 0.0057 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0071 - acc: 0.0014 - val_loss: 0.0276 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0071 - acc: 0.0014 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0047 - acc: 0.0014 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0031 - acc: 0.0014 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0040 - acc: 0.0014 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0029 - acc: 0.0014 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0022 - acc: 0.0014 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0022 - acc: 0.0014 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0020 - acc: 0.0014 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0019 - acc: 0.0014 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0019 - acc: 0.0014 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0017 - acc: 0.0014 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0018 - acc: 0.0014 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0017 - acc: 0.0014 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0016 - acc: 0.0014 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1410/1410 [==============================] - 6s 5ms/step - loss: 0.0017 - acc: 0.0014 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0017 - acc: 0.0014 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0017 - acc: 0.0014 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0017 - acc: 0.0014 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0017 - acc: 0.0014 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0017 - acc: 0.0014 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0016 - acc: 0.0014 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0016 - acc: 0.0014 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0016 - acc: 0.0014 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0016 - acc: 0.0014 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0016 - acc: 0.0014 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0016 - acc: 0.0014 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1410/1410 [==============================] - 6s 5ms/step - loss: 0.0016 - acc: 0.0014 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1410/1410 [==============================] - 7s 5ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1410/1410 [==============================] - 7s 5ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1410/1410 [==============================] - 7s 5ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f6deacb208>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=90,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174, 1)\n"
     ]
    }
   ],
   "source": [
    "diff=[]\n",
    "ratio=[]\n",
    "p = model.predict(X_test)\n",
    "print (p.shape) #for each data index in test data\n",
    "for u in range(len(y_test)):\n",
    "    # pr = prediction day u\n",
    "    pr = p[u][0]\n",
    "    # (y_test day u / pr) - 1\n",
    "    ratio.append((y_test[u]/pr)-1)\n",
    "    diff.append(abs(y_test[u]- pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00067 MSE (0.03 RMSE)\n",
      "Test Score: 0.00117 MSE (0.03 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00066958577308985667, 0.0011673327535390854)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n",
    "\n",
    "\n",
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('prices-split-adjusted.csv', index_col = 0)\n",
    "df[\"adj close\"] = df.close # Moving close to the last column\n",
    "df.drop(['close'], 1, inplace=True) # Moving close to the last column\n",
    "df = df[df.symbol == 'AAPL']\n",
    "df.drop(['symbol'],1,inplace=True)\n",
    "\n",
    "# Bug fixed at here, please update the denormalize function to this one\n",
    "def denormalize(df, normalized_value): \n",
    "    df = df['adj close'].values.reshape(-1,1)\n",
    "    normalized_value = normalized_value.reshape(-1,1)\n",
    "    \n",
    "    #return df.shape, p.shape\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    a = min_max_scaler.fit_transform(df)\n",
    "    new = min_max_scaler.inverse_transform(normalized_value)\n",
    "    return new\n",
    "\n",
    "newp = denormalize(df, p)\n",
    "newy_test = denormalize(df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00067 MSE (0.03 RMSE)\n",
      "Test Score: 0.00117 MSE (0.03 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00066958577308985667, 0.0011673327535390854)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n",
    "\n",
    "\n",
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting: Prediction vs Actual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzlGhVxRFiXNU6BVFUeIcFXpFUZQ45/8BnJuE3z+bUxMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ead78a0c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(newp,color='red', label='Prediction')\n",
    "plt.plot(newy_test,color='blue', label='Actual')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The predicted value is very close to actual value and hence, the prediction model of RNN using Keras works pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B - Activation Function:\n",
    " Change the activation function. How does it effect the accuracy?\n",
    " \n",
    " LeakyReLU and PReLU are slightly better versions of ReLU activation function and we should expect slightly higher accuracy.\n",
    "### 1. Using LeakyReLU as activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.advanced_activations import LeakyReLU, PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    d = 0.3\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    #model.add(Dense(32,kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(1,kernel_initializer=\"uniform\",activation='linear'))\n",
    "    model.add(LeakyReLU(alpha=.1)) \n",
    "        \n",
    "    start = time.time()\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation Time :  0.030521631240844727\n"
     ]
    }
   ],
   "source": [
    "model = build_model([5,window,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1410 samples, validate on 157 samples\n",
      "Epoch 1/20\n",
      "1410/1410 [==============================] - 8s 6ms/step - loss: 0.1607 - acc: 7.0922e-04 - val_loss: 0.0196 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0228 - acc: 0.0014 - val_loss: 0.0292 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0217 - acc: 0.0014 - val_loss: 0.0432 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0102 - acc: 0.0014 - val_loss: 0.0107 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0120 - acc: 0.0014 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0041 - acc: 0.0014 - val_loss: 0.0186 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0078 - acc: 0.0014 - val_loss: 0.0047 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0026 - acc: 0.0014 - val_loss: 0.0076 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0046 - acc: 0.0014 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0023 - acc: 0.0014 - val_loss: 0.0048 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0033 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0020 - acc: 0.0014 - val_loss: 0.0053 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0025 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0020 - acc: 0.0014 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0018 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0019 - acc: 0.0014 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0016 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0017 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0016 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0016 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ea814cc390>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=20,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00074 MSE (0.03 RMSE)\n",
      "Test Score: 0.00120 MSE (0.03 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00073867990914969954, 0.0011951407048337419)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n",
    "\n",
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using PReLU as the activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    d = 0.3\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    #model.add(Dense(32,kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(1,kernel_initializer=\"uniform\",activation='linear'))\n",
    "    act = keras.layers.advanced_activations.PReLU(init='one', weights=None)\n",
    "    model.add(act) \n",
    "        \n",
    "    start = time.time()\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation Time :  0.030521154403686523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `PReLU` call to the Keras 2 API: `PReLU(weights=None, alpha_initializer=\"one\")`\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "model = build_model([5,window,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1410 samples, validate on 157 samples\n",
      "Epoch 1/20\n",
      "1410/1410 [==============================] - 8s 5ms/step - loss: 0.1596 - acc: 7.0922e-04 - val_loss: 0.0254 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0208 - acc: 0.0014 - val_loss: 0.0251 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0179 - acc: 0.0014 - val_loss: 0.0334 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0081 - acc: 0.0014 - val_loss: 0.0122 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0109 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0044 - acc: 0.0014 - val_loss: 0.0174 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0069 - acc: 0.0014 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0029 - acc: 0.0014 - val_loss: 0.0072 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "1410/1410 [==============================] - 7s 5ms/step - loss: 0.0039 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0025 - acc: 0.0014 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0025 - acc: 0.0014 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0023 - acc: 0.0014 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0018 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0020 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0017 - acc: 0.0014 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0018 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0017 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0016 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0016 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ea8612c828>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=20,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00077 MSE (0.03 RMSE)\n",
      "Test Score: 0.00120 MSE (0.03 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00077128704074454029, 0.0011958439323258982)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n",
    "\n",
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "Applying LeakyReLU and PReLU as activation function, we get a little better accuracy for test case. However, it doesnt change much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C - Cost Function:\n",
    "A cost function is a measure of \"how good\" a neural network did with respect to it's given training sample and the expected output. It also may depend on variables such as weights and biases.\n",
    "\n",
    "### 1. Using hinge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    d = 0.3\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(Dense(32,kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(1,kernel_initializer=\"uniform\",activation='linear'))\n",
    "    #model.add(LeakyReLU(alpha=.1)) \n",
    "        \n",
    "    start = time.time()\n",
    "    model.compile(loss='hinge',optimizer='adam', metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation Time :  0.039559364318847656\n"
     ]
    }
   ],
   "source": [
    "model = build_model([5,window,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1410 samples, validate on 157 samples\n",
      "Epoch 1/20\n",
      "1410/1410 [==============================] - 10s 7ms/step - loss: 0.9287 - acc: 7.0922e-04 - val_loss: 0.2306 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.4918 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.1211 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0600 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0498 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0448 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0404 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0370 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0340 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0314 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0294 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0275 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0259 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0243 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0233 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0225 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0218 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0201 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0200 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27c19d912e8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=20,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.01754 MSE (0.13 RMSE)\n",
      "Test Score: 0.00000 MSE (0.00 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.017535012964237703, 0.0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n",
    "\n",
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using Squared hinge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    d = 0.3\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(Dense(32,kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(1,kernel_initializer=\"uniform\",activation='linear'))\n",
    "    #model.add(LeakyReLU(alpha=.1)) \n",
    "        \n",
    "    start = time.time()\n",
    "    model.compile(loss='squared_hinge',optimizer='adam', metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation Time :  0.03202319145202637\n"
     ]
    }
   ],
   "source": [
    "model = build_model([5,window,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1410 samples, validate on 157 samples\n",
      "Epoch 1/20\n",
      "1410/1410 [==============================] - 8s 6ms/step - loss: 0.9882 - acc: 7.0922e-04 - val_loss: 0.8728 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.8755 - acc: 7.0922e-04 - val_loss: 0.3401 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.5326 - acc: 0.0014 - val_loss: 0.0175 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.2724 - acc: 7.0922e-04 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.1560 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.1104 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0837 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0667 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0561 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0491 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0436 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0396 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0358 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0334 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0308 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0292 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0273 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0257 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0247 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0232 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f69ef48518>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=20,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.02002 MSE (0.14 RMSE)\n",
      "Test Score: 0.00000 MSE (0.00 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.020022971354705481, 0.0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n",
    "\n",
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:\n",
    "Using both hinge and squared hinge as the cost function, the test case accuracy is the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D - Epochs:\n",
    "An epoch is the entire training data exposed to the network, batch-by-batch. As no. of epoch increases, one would expect the test accuracy to increase. However, avoid overfitting the model.\n",
    "\n",
    "Change the number of epochs initialization. How does it effect the accuracy?\n",
    "\n",
    "### 1. Epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1410 samples, validate on 157 samples\n",
      "Epoch 1/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0016 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1410/1410 [==============================] - 7s 5ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 9.9158e-04 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 9.7744e-04 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 9.6444e-04 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1410/1410 [==============================] - 6s 5ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 9.8662e-04 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 9.9544e-04 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 9.5654e-04 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 9.4481e-04 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 9.8483e-04 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 9.3266e-04 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 9.2935e-04 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 9.6269e-04 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ea83cd27f0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=50,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00065 MSE (0.03 RMSE)\n",
      "Test Score: 0.00099 MSE (0.03 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00065125050584844476, 0.00099131149566336271)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n",
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Epoch =20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1410 samples, validate on 157 samples\n",
      "Epoch 1/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0014 - acc: 0.0014 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27c56fdaf98>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=20,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00075 MSE (0.03 RMSE)\n",
      "Test Score: 0.00137 MSE (0.04 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00075324305526207336, 0.0013732683982689404)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n",
    "\n",
    "\n",
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:\n",
    "As the number of epoch increases, the test accuracy also increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E - Gradient estimation:\n",
    "Change the gradient estimation. How does it effect the accuracy?\n",
    "\n",
    "### 1. Using RMSDrop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    d = 0.3\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "                \n",
    "    model.add(Dense(32,kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(1,kernel_initializer=\"uniform\",activation='linear'))\n",
    "    \n",
    "    start = time.time()\n",
    "    model.compile(loss='mse',optimizer='RMSProp', metrics=['accuracy'])\n",
    "    keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation Time :  0.043030500411987305\n"
     ]
    }
   ],
   "source": [
    "model = build_model([5,window,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1410 samples, validate on 157 samples\n",
      "Epoch 1/20\n",
      "1410/1410 [==============================] - 8s 6ms/step - loss: 0.1801 - acc: 0.0014 - val_loss: 0.3921 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0853 - acc: 7.0922e-04 - val_loss: 0.2800 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0603 - acc: 7.0922e-04 - val_loss: 0.0664 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0290 - acc: 0.0014 - val_loss: 0.0047 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0288 - acc: 0.0014 - val_loss: 0.0321 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0139 - acc: 0.0014 - val_loss: 0.0108 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0226 - acc: 0.0014 - val_loss: 0.0353 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0184 - acc: 0.0014 - val_loss: 0.0057 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0139 - acc: 0.0014 - val_loss: 0.0313 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0153 - acc: 0.0014 - val_loss: 0.0097 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0178 - acc: 0.0014 - val_loss: 0.0497 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0183 - acc: 0.0014 - val_loss: 0.0078 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0117 - acc: 0.0014 - val_loss: 0.0185 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0090 - acc: 0.0014 - val_loss: 0.0112 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0133 - acc: 0.0014 - val_loss: 0.0320 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0157 - acc: 0.0014 - val_loss: 0.0060 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0119 - acc: 0.0014 - val_loss: 0.0383 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0159 - acc: 0.0014 - val_loss: 0.0169 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0124 - acc: 0.0014 - val_loss: 0.0146 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0071 - acc: 0.0014 - val_loss: 0.0119 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f6823d1b38>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=20,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00749 MSE (0.09 RMSE)\n",
      "Test Score: 0.01334 MSE (0.12 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0074913287582856576, 0.013338576437605694)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n",
    "\n",
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using Stochiotic Gradient Descent(SGD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    d = 0.3\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "                \n",
    "    model.add(Dense(32,kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(1,kernel_initializer=\"uniform\",activation='linear'))\n",
    "    \n",
    "    start = time.time()\n",
    "    model.compile(loss='mse',optimizer='SGD', metrics=['accuracy'])\n",
    "    keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation Time :  0.04102730751037598\n"
     ]
    }
   ],
   "source": [
    "model = build_model([5,window,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1410 samples, validate on 157 samples\n",
      "Epoch 1/20\n",
      "1410/1410 [==============================] - 8s 5ms/step - loss: 0.2509 - acc: 7.0922e-04 - val_loss: 0.5484 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.2294 - acc: 7.0922e-04 - val_loss: 0.5125 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.2105 - acc: 7.0922e-04 - val_loss: 0.4801 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.1938 - acc: 7.0922e-04 - val_loss: 0.4504 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.1789 - acc: 7.0922e-04 - val_loss: 0.4233 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.1657 - acc: 7.0922e-04 - val_loss: 0.3986 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.1541 - acc: 7.0922e-04 - val_loss: 0.3761 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.1438 - acc: 7.0922e-04 - val_loss: 0.3554 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.1347 - acc: 7.0922e-04 - val_loss: 0.3365 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.1267 - acc: 7.0922e-04 - val_loss: 0.3193 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.1196 - acc: 7.0922e-04 - val_loss: 0.3036 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.1133 - acc: 7.0922e-04 - val_loss: 0.2891 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.1078 - acc: 7.0922e-04 - val_loss: 0.2757 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.1028 - acc: 7.0922e-04 - val_loss: 0.2635 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0985 - acc: 7.0922e-04 - val_loss: 0.2522 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0945 - acc: 7.0922e-04 - val_loss: 0.2419 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0912 - acc: 7.0922e-04 - val_loss: 0.2325 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0883 - acc: 7.0922e-04 - val_loss: 0.2237 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0856 - acc: 7.0922e-04 - val_loss: 0.2157 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0832 - acc: 7.0922e-04 - val_loss: 0.2081 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f68d709c50>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=20,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.09433 MSE (0.31 RMSE)\n",
      "Test Score: 0.19051 MSE (0.44 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.094333360255734508, 0.19051299047196049)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n",
    "\n",
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:\n",
    "The test accuracy decreases.\n",
    "### RMSE:\n",
    "#### For RMSDrop:\n",
    "- Train = 0.007\n",
    "- Test = 0.013\n",
    "\n",
    "#### For SGD:\n",
    "- Train =0.094\n",
    "- Test = 0.19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F - Network Architecture:\n",
    "Change the network architecture. How does it effect the accuracy?\n",
    "How does it effect how quickly the network plateaus?\n",
    "Various forms of network architecture:\n",
    "- Number of layers\n",
    "- Size of each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. No. of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    d = 0.3 #Dropout\n",
    "    #Dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data.\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #hidden layer\n",
    "    #model.add(LSTM(hidden_nodes, input_shape=(timesteps, input_dim)))\n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "    \n",
    "    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "     \n",
    "    #output layer    \n",
    "    model.add(Dense(32,kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(1,kernel_initializer=\"uniform\",activation='linear'))\n",
    "        \n",
    "    start = time.time()\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1410 samples, validate on 157 samples\n",
      "Epoch 1/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0016 - acc: 0.0014 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "1410/1410 [==============================] - 7s 5ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 9.8745e-04 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0015 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f6e2412cf8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=20,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00069 MSE (0.03 RMSE)\n",
      "Test Score: 0.00134 MSE (0.04 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00068764324592255255, 0.0013435743640420488)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n",
    "\n",
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Size of the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    d = 0.3 #Dropout\n",
    "    #Dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data.\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(32, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(32, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "     \n",
    "    #output layer    \n",
    "    model.add(Dense(32,kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(1,kernel_initializer=\"uniform\",activation='linear'))\n",
    "        \n",
    "    start = time.time()\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1410 samples, validate on 157 samples\n",
      "Epoch 1/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 9.7355e-04 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 9.7961e-04 - acc: 0.0014 - val_loss: 8.7137e-04 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 9.1163e-04 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 9.7570e-04 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 8.4086e-04 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 8.4792e-04 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 8.9093e-04 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 8.3328e-04 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0012 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0013 - acc: 0.0014 - val_loss: 8.1214e-04 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0010 - acc: 0.0014 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 9.6380e-04 - acc: 0.0014 - val_loss: 9.9661e-04 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f6e2412940>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=20,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00059 MSE (0.02 RMSE)\n",
      "Test Score: 0.00098 MSE (0.03 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00058570810352661902, 0.00098301268526588452)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n",
    "\n",
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:\n",
    "The mse for test and train score decreases and hence accuracy increases slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part G - Network initialization:\n",
    "Change the network initialization. How does it effect the accuracy?\n",
    "\n",
    "How does it effect how quickly the network plateaus?\n",
    "\n",
    "### 1. Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    d = 0.3 #Dropout\n",
    "    #Dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data.\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(32, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(32, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "     \n",
    "    #output layer    \n",
    "    model.add(Dense(32,kernel_initializer=\"gaussian\",activation='relu'))        \n",
    "    model.add(Dense(1,kernel_initializer=\"gaussian\",activation='linear'))\n",
    "        \n",
    "    start = time.time()\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_model([5,window,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1410 samples, validate on 157 samples\n",
      "Epoch 1/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 9.6158e-04 - acc: 0.0014 - val_loss: 9.6139e-04 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0010 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 9.8435e-04 - acc: 0.0014 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0010 - acc: 0.0014 - val_loss: 9.9831e-04 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0010 - acc: 0.0014 - val_loss: 7.9071e-04 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 9.8446e-04 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 9.3705e-04 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "1410/1410 [==============================] - 6s 5ms/step - loss: 9.6076e-04 - acc: 0.0014 - val_loss: 7.9478e-04 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0010 - acc: 0.0014 - val_loss: 8.5043e-04 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 9.4181e-04 - acc: 0.0014 - val_loss: 8.4844e-04 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 9.8109e-04 - acc: 0.0014 - val_loss: 7.6481e-04 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 9.9944e-04 - acc: 0.0014 - val_loss: 9.7714e-04 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 9.9324e-04 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0010 - acc: 0.0014 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 8.7401e-04 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 8.6179e-04 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "1410/1410 [==============================] - 5s 4ms/step - loss: 9.6972e-04 - acc: 0.0014 - val_loss: 9.7383e-04 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 0.0010 - acc: 0.0014 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "1410/1410 [==============================] - 5s 3ms/step - loss: 9.6541e-04 - acc: 0.0014 - val_loss: 8.8207e-04 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "1410/1410 [==============================] - 6s 4ms/step - loss: 0.0011 - acc: 0.0014 - val_loss: 7.7489e-04 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f6e23fdbe0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=20,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00058 MSE (0.02 RMSE)\n",
      "Test Score: 0.00076 MSE (0.03 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00057898703924675623, 0.00076276013606238642)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n",
    "\n",
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using Xavier Uniform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Flatten\n",
    "def build_model(layers):\n",
    "    d = 0.3 #Dropout\n",
    "    #Dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data.\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(32, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(32, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "     \n",
    "    #output layer    \n",
    "    model.add(Dense(32,kernel_initializer=\"glorot_uniform\",activation='relu'))        \n",
    "    model.add(Dense(1,kernel_initializer=\"glorot_uniform\",activation='linear'))\n",
    "        \n",
    "    start = time.time()\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation Time :  0.031039953231811523\n"
     ]
    }
   ],
   "source": [
    "model = build_model([5,window,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1410 samples, validate on 157 samples\n",
      "Epoch 1/20\n",
      "1410/1410 [==============================] - 4s 3ms/step - loss: 0.1295 - acc: 0.0014 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1410/1410 [==============================] - 1s 415us/step - loss: 0.0324 - acc: 0.0014 - val_loss: 0.0535 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "1410/1410 [==============================] - 1s 405us/step - loss: 0.0203 - acc: 0.0014 - val_loss: 0.0097 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "1410/1410 [==============================] - 1s 373us/step - loss: 0.0119 - acc: 0.0014 - val_loss: 0.0457 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "1410/1410 [==============================] - 1s 415us/step - loss: 0.0197 - acc: 0.0014 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "1410/1410 [==============================] - 1s 396us/step - loss: 0.0089 - acc: 0.0014 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "1410/1410 [==============================] - 1s 380us/step - loss: 0.0065 - acc: 0.0014 - val_loss: 0.0133 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "1410/1410 [==============================] - 1s 394us/step - loss: 0.0093 - acc: 0.0014 - val_loss: 0.0064 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "1410/1410 [==============================] - 1s 383us/step - loss: 0.0053 - acc: 0.0014 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "1410/1410 [==============================] - 1s 390us/step - loss: 0.0046 - acc: 0.0014 - val_loss: 0.0062 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "1410/1410 [==============================] - 1s 389us/step - loss: 0.0059 - acc: 0.0014 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "1410/1410 [==============================] - 1s 385us/step - loss: 0.0042 - acc: 0.0014 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "1410/1410 [==============================] - 1s 385us/step - loss: 0.0043 - acc: 0.0014 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "1410/1410 [==============================] - 1s 392us/step - loss: 0.0044 - acc: 0.0014 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "1410/1410 [==============================] - 1s 382us/step - loss: 0.0041 - acc: 0.0014 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "1410/1410 [==============================] - 1s 412us/step - loss: 0.0036 - acc: 0.0014 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "1410/1410 [==============================] - 1s 380us/step - loss: 0.0041 - acc: 0.0014 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "1410/1410 [==============================] - 1s 378us/step - loss: 0.0036 - acc: 0.0014 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "1410/1410 [==============================] - 1s 395us/step - loss: 0.0037 - acc: 0.0014 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "1410/1410 [==============================] - 1s 402us/step - loss: 0.0036 - acc: 0.0014 - val_loss: 0.0023 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f6a92ac320>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=20,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00131 MSE (0.04 RMSE)\n",
      "Test Score: 0.00249 MSE (0.05 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0013080054684066651, 0.0024930840885352299)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n",
    "\n",
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:\n",
    "The mse for test and train score decreases in case of Gaussian while it increases in case of Xavier Glorot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "- https://www.kaggle.com/benjibb/lstm-stock-prediction-20170507\n",
    "- https://machinelearningmastery.com/improve-deep-learning-performance/\n",
    "- https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
